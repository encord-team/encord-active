[DATA]
train_data_folder = '/path/to/train/data/folder'
train_ann = '/path/to/train/annotation/file'

validation_data_folder = '/path/to/validation/data/folder'
validation_ann = '/path/to/validation/annotation/file'

[LOGGING]
# calculating mAP takes too much time, so we calculate it in every nth step
# calculating train set mAP is optional, disabling it will significantly increase training time
wandb_enabled = True
wandb_project = wandb_project_name
log_train_map = False
performance_tracking_interval = 2

[TRAIN]
# lr_scheduling patience and early_stopping_thresh are based on the performance_tracking_interval
learning_rate = 0.0001
batch_size = 10
max_epoch = 100
num_worker = 4
use_lr_scheduler = False
lr_scheduler_patience = 3
early_stopping_thresh = 7

[INFERENCE]
# === Fill this part after the training ===
# target_data_folder and target_ann: Path to root image folder and COCO annotations for the dataset
# wandb_id: The unique wandb id that can be found in wandb platform, we append this to pickle file to not to confuse prediction files
# model_checkpoint_path: Path to model checkpoint file with .ckpt extension
# ontology_filepath: Encord ontology file, can be found in the project folder
# confidence_threshold: Score threshold for the each instance (Mask-RCNN outputs score for each prediction)
target_data_folder = /path/to/target/folder
target_ann = /path/to/target/annotation/file
wandb_id = wandb_unique_id
model_checkpoint_path = /path/to/model/checkpoint
ontology_filepath = /path/to/ontology.json
confidence_threshold = 0.5
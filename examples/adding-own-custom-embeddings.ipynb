{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f735c771",
   "metadata": {},
   "source": [
    "# Custom Embeddings\n",
    "\n",
    "Encord Active have three different types of embeddings.\n",
    "\n",
    "1. _Image embeddings:_ are general for each image / frame in the dataset\n",
    "2. _Classification embeddings:_ are associated to specific frame level classifications\n",
    "3. _Object embeddings:_ are associated to specific objects like polygons of bounding boxes\n",
    "\n",
    "If you like, you can \"swap out\" these embeddings with your own by following the steps in this notebook.\n",
    "\n",
    "There are two sections in the notebook. One for the image embeddings and one for the objects.\n",
    "If you have classifications in your project, you should run \n",
    "\n",
    "```\n",
    "encord-active metric run \"Image-level Annotation Quality\"\n",
    "```\n",
    "\n",
    "This will take the image level embeddings that you provided and also associate them to the classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74cdeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from encord_active.lib.common.iterator import DatasetIterator, Iterator\n",
    "from encord_active.lib.embeddings.dimensionality_reduction import (\n",
    "    generate_2d_embedding_data,\n",
    ")\n",
    "from encord_active.lib.embeddings.types import LabelEmbedding\n",
    "from encord_active.lib.metrics.types import EmbeddingType\n",
    "from encord_active.lib.project.project_file_structure import ProjectFileStructure\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "def load_my_model() -> torch.nn.Module:\n",
    "    ...  # <- HERE: Edit here to return your model\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    return (\n",
    "        ToTensor()\n",
    "    )  # <- HERE: If you have any specific transforms to apply to PIL images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e58bc8",
   "metadata": {},
   "source": [
    "## Examle of Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_cnn_image_embeddings(iterator: Iterator) -> List[LabelEmbedding]:\n",
    "    model = load_my_model()\n",
    "    transform = get_transform()\n",
    "\n",
    "    collections: List[LabelEmbedding] = []\n",
    "    for data_unit, image in iterator.iterate(desc=\"Embedding image data.\"):\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        image_pil = image.convert(\"RGB\")\n",
    "        image = transform(image_pil)\n",
    "\n",
    "        # START Embedding\n",
    "        embedding = model(image)  # <- HERE - your logic for embedding data.\n",
    "\n",
    "        if embedding is None:\n",
    "            continue\n",
    "\n",
    "        embedding = embedding.flatten().detach().numpy()  # <- should be a [d,] array.\n",
    "        # End Embedding\n",
    "\n",
    "        entry = LabelEmbedding(\n",
    "            url=data_unit[\"data_link\"],\n",
    "            label_row=iterator.label_hash,\n",
    "            data_unit=data_unit[\"data_hash\"],\n",
    "            frame=iterator.frame,\n",
    "            dataset_title=iterator.dataset_title,\n",
    "            embedding=embedding,\n",
    "            labelHash=None,\n",
    "            lastEditedBy=None,\n",
    "            featureHash=None,\n",
    "            name=None,\n",
    "            classification_answers=None,\n",
    "        )\n",
    "        collections.append(entry)\n",
    "\n",
    "    return collections\n",
    "\n",
    "\n",
    "project = Path(\"/path/to/your/project/root\")  # <- HERE: Path to the Encord Project\n",
    "pfs = ProjectFileStructure(project)\n",
    "\n",
    "iterator = DatasetIterator(project)\n",
    "embeddings = generate_cnn_image_embeddings(iterator)\n",
    "out_file = prfs.get_embeddings_file(EmbeddingType.IMAGE)\n",
    "\n",
    "with out_file.open(\"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "generate_2d_embedding_data(EmbeddingType.IMAGE, project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fde49",
   "metadata": {},
   "source": [
    "## Example of Object Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddaa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encord_active.lib.common.utils import get_bbox_from_encord_label_object\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_cnn_object_embeddings(iterator: Iterator) -> List[LabelEmbedding]:\n",
    "    model = get_model()\n",
    "    transform = get_transform()\n",
    "\n",
    "    embeddings: List[LabelEmbedding] = []\n",
    "    for data_unit, image in iterator.iterate(desc=\"Embedding object data.\"):\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        image_pil = image.convert(\"RGB\")\n",
    "        image = transform(image_pil)\n",
    "        \n",
    "        for obj in data_unit[\"labels\"].get(\"objects\", []):\n",
    "            if obj[\"shape\"] in [\n",
    "                ObjectShape.POLYGON.value,\n",
    "                ObjectShape.BOUNDING_BOX.value,\n",
    "                ObjectShape.ROTATABLE_BOUNDING_BOX.value,\n",
    "            ]:\n",
    "                # Crops images tightly around object\n",
    "                out = get_bbox_from_encord_label_object( \n",
    "                    obj,\n",
    "                    image.shape[2],\n",
    "                    image.shape[1],\n",
    "                )\n",
    "\n",
    "                if out is None:\n",
    "                    continue\n",
    "                \n",
    "                x, y, w, h = out\n",
    "                img_patch = image[:, y : y + h, x : x + w]\n",
    "                \n",
    "                # Compute embeddings\n",
    "                embedding = model(img_patch)\n",
    "                embedding = embedding.flatten().detach().numpy()  # <- should be a [d,] array.\n",
    "\n",
    "                last_edited_by = obj[\"lastEditedBy\"] if \"lastEditedBy\" in obj.keys() else obj[\"createdBy\"]\n",
    "                entry = LabelEmbedding(\n",
    "                    url=data_unit[\"data_link\"],\n",
    "                    label_row=iterator.label_hash,\n",
    "                    data_unit=data_unit[\"data_hash\"],\n",
    "                    frame=iterator.frame,\n",
    "                    labelHash=obj[\"objectHash\"],\n",
    "                    lastEditedBy=last_edited_by,\n",
    "                    featureHash=obj[\"featureHash\"],\n",
    "                    name=obj[\"name\"],\n",
    "                    dataset_title=iterator.dataset_title,\n",
    "                    embedding=embedding,\n",
    "                    classification_answers=None,\n",
    "                )\n",
    "\n",
    "                embeddings.append(entry)\n",
    "\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = generate_cnn_object_embeddings(iterator)\n",
    "out_file = pfs.get_embeddings_file(EmbeddingType.OBJECT)\n",
    "\n",
    "with out_file.open(\"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "generate_2d_embedding_data(EmbeddingType.OBJECT, project)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

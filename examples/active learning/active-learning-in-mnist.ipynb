{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from encord_active.lib.common.iterator import DatasetIterator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from encord_active.lib.project.project_file_structure import ProjectFileStructure\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from encord_active.lib.metrics.execute import execute_metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "config = yaml.safe_load(Path(\"config.yaml\").read_text())\n",
    "project_dir = Path(config[\"project_dir\"])\n",
    "project_dir_train = project_dir # todo change project_dir in the config file to train and test\n",
    "project_fs_train = ProjectFileStructure(project_dir_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def get_data_hashes_from_project(project_dir: Path, subset_size=None):\n",
    "    iterator = DatasetIterator(project_dir, subset_size)\n",
    "    data_hashes = [(iterator.label_hash, iterator.du_hash) for data_unit, img_pth in iterator.iterate()]\n",
    "    return data_hashes\n",
    "\n",
    "def get_data_from_data_hashes(project_fs: ProjectFileStructure, data_hashes: list[tuple[str, str]]):\n",
    "    image_arrays, class_labels = zip(*(get_data_sample(project_fs, data_hash) for data_hash in data_hashes))\n",
    "    return list(image_arrays), list(class_labels)\n",
    "\n",
    "def get_data_sample(project_fs: ProjectFileStructure, data_hash: tuple[str, str]):\n",
    "    label_hash, du_hash = data_hash\n",
    "    lr_struct = project_fs.label_row_structure(label_hash)\n",
    "    \n",
    "    # get classification label\n",
    "    label_row = json.loads(lr_struct.label_row_file.read_text())\n",
    "    class_label = get_classification_label(label_row, du_hash, class_name=\"Classification\")\n",
    "    \n",
    "    # get image\n",
    "    image_path = lr_struct.images_dir / f\"{du_hash}.{label_row['data_units'][du_hash]['data_type'].split('/')[-1]}\"\n",
    "    image_array = np.asarray(Image.open(image_path)).flatten()\n",
    "    \n",
    "    return image_array, class_label\n",
    "\n",
    "def get_classification_label(label_row, du_hash: str, class_name: str):\n",
    "    data_unit = label_row[\"data_units\"][du_hash]\n",
    "    filtered_class = [_class for _class in data_unit[\"labels\"][\"classifications\"] if _class[\"name\"] == class_name]\n",
    "    if len(filtered_class) == 0:\n",
    "        return None\n",
    "    class_hash = filtered_class[0][\"classificationHash\"]\n",
    "    # check if this is the same for text classification instead of radio button classification\n",
    "    class_label = label_row[\"classification_answers\"][class_hash][\"classifications\"][0][\"answers\"][0][\"name\"]\n",
    "    return class_label\n",
    "\n",
    "def train_model(X_train, y_train, model=None):\n",
    "    # use logistic regression model as a dummy model example\n",
    "    if model is None:\n",
    "        model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def get_model_accuracy(X_test, y_test, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def get_n_best_ranked_data_samples(project_fs: ProjectFileStructure, data_hashes, n, acq_func_instance, rank_by: str):\n",
    "    execute_metrics([acq_func_instance], data_dir=project_fs.project_dir)\n",
    "    unique_acq_func_name = acq_func_instance.metadata.get_unique_name()\n",
    "    acq_func_results = pd.read_csv(project_fs.metrics / f\"{unique_acq_func_name}.csv\")\n",
    "        \n",
    "    # filter acquisition function results to only contain data samples specified in data_hashes\n",
    "    str_data_hashes = tuple(f\"{label_hash}_{du_hash}\" for label_hash, du_hash in data_hashes)\n",
    "    filtered_results = acq_func_results[acq_func_results['identifier'].str.startswith(str_data_hashes, na=False)]\n",
    "    \n",
    "    if rank_by == \"asc\": # get the first n data samples if they were sorted by ascending score order\n",
    "        best_n = filtered_results[[\"identifier\", \"score\"]].nsmallest(n, \"score\", keep=\"first\")[\"identifier\"]\n",
    "    elif rank_by == \"desc\":  # get the first n data samples if they were sorted by descending score order\n",
    "        best_n = filtered_results[[\"identifier\", \"score\"]].nlargest(n, \"score\", keep=\"first\")[\"identifier\"]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return [get_data_hash_from_identifier(identifier) for identifier in best_n]\n",
    "    \n",
    "def get_data_hash_from_identifier(identifier: str):\n",
    "    return tuple(identifier.split(\"_\", maxsplit=2)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data hashes from train project\n",
    "subset_size = None\n",
    "data_hashes_train = get_data_hashes_from_project(project_dir_train, subset_size)\n",
    "\n",
    "# shuffle data hashes\n",
    "data_hashes_train = shuffle(data_hashes_train, random_state=42)\n",
    "\n",
    "print(f\"Train dataset size: {len(data_hashes_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "project_dir_test = project_dir  # todo change this for the real test project\n",
    "project_fs_test = ProjectFileStructure(project_dir_test)\n",
    "data_hashes_test = get_data_hashes_from_project(project_dir_test)\n",
    "X_test, y_test = get_data_from_data_hashes(project_fs_test, data_hashes_test)\n",
    "print(f\"Test dataset size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set configuration variables needed for the active learning workflow (move to config.yaml)\n",
    "initial_data_amount = 20  # initial amount of labeled data\n",
    "n_iterations = 10 # number of iterations of the active learning paradigm\n",
    "batch_size_to_label = 5 # number of data samples labeled between AL iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load common acquisition functions (for active learning)\n",
    "from encord_active.lib.metrics.acquisition_functions import Entropy, LeastConfidence, Margin, Variance\n",
    "\n",
    "# use 'asc' (ascending) and 'desc' (descending) ordering for later selection of k highest ranked data samples\n",
    "acq_funcs = [(Entropy, \"desc\"), (LeastConfidence, \"desc\"), (Margin, \"asc\"), (Variance, \"desc\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logger = defaultdict(dict)\n",
    "for acq_func, rank_order in acq_funcs:\n",
    "    print(f\"Analyzing acquisition function: {acq_func.__name__}\")\n",
    "    \n",
    "    # mockup of the initial labeling phase\n",
    "    labeled_data_hashes_train = data_hashes_train[:initial_data_amount]\n",
    "    unlabeled_data_hashes_train = set(data_hashes_train[initial_data_amount:])\n",
    "    \n",
    "    X, y = get_data_from_data_hashes(project_fs_train, labeled_data_hashes_train)\n",
    "    model = train_model(X, y)\n",
    "    accuracy_logger[acq_func.__name__][0] = get_model_accuracy(X_test, y_test, model)\n",
    "    for it in tqdm(range(1, n_iterations + 1)):\n",
    "        acq_func_instance = acq_func(model)\n",
    "        data_to_label_next = get_n_best_ranked_data_samples(project_fs_train, unlabeled_data_hashes_train, batch_size_to_label, acq_func_instance, rank_by=rank_order)\n",
    "        \n",
    "        # mockup of the labeling phase\n",
    "        X_new, y_new = get_data_from_data_hashes(project_fs_train, data_to_label_next)\n",
    "        unlabeled_data_hashes_train.difference_update(data_to_label_next)\n",
    "        \n",
    "        X.extend(X_new)\n",
    "        y.extend(y_new)\n",
    "        model = train_model(X, y)\n",
    "        accuracy_logger[acq_func.__name__][it] = get_model_accuracy(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for acq_func_name, points in accuracy_logger.items():\n",
    "    xs, ys = zip(*points.items())\n",
    "    plt.plot(xs, ys, label=acq_func_name)\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Model Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

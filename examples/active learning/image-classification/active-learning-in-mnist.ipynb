{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning in MNIST dataset using Random Forest\n",
    "\n",
    "**Prerequisites**:\n",
    "You need to have encord-active [installed](https://docs.encord.com/docs/active-installation).\n",
    "\n",
    "This notebook shows you how to plug your model in Encord Active and use it to run some iterations of the active learning cycle in the MNIST sandbox projects.\n",
    "\n",
    "It follows five steps:\n",
    "1. Download the MNIST sandbox projects.\n",
    "2. Setup the model.\n",
    "3. Select the acquisition functions.\n",
    "4. Simulate the active learning workflow.\n",
    "5. Compare and visualize the simulation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the MNIST sandbox projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from encord_active.lib.common.active_learning import get_data_hashes_from_project\n",
    "from encord_active.lib.project.project_file_structure import ProjectFileStructure\n",
    "from encord_active.lib.project.sandbox_projects import fetch_prebuilt_project\n",
    "\n",
    "def init_project_data(project_name, subset_size):\n",
    "    # Choose where to store the project\n",
    "    project_path = Path.cwd() / project_name\n",
    "    # Download the project\n",
    "    fetch_prebuilt_project(project_name, project_path)\n",
    "    project_fs = ProjectFileStructure(project_path)\n",
    "    # Select data from the project\n",
    "    data_hashes = get_data_hashes_from_project(project_fs, subset_size)\n",
    "    return project_fs, data_hashes\n",
    "\n",
    "project_fs_train, data_hashes_train = init_project_data(\"[open-source][train]-mnist-dataset\", subset_size=5000)\n",
    "project_fs_test, data_hashes_test = init_project_data(\"[open-source][test]-mnist-dataset\", subset_size=1000)\n",
    "\n",
    "print(f\"Train dataset size: {len(data_hashes_train)}\")\n",
    "print(f\"Test dataset size: {len(data_hashes_test)}\")\n",
    "\n",
    "initial_data_amount = 500\n",
    "n_iterations = 15\n",
    "batch_size_to_label = 300\n",
    "class_name = \"digit\" # name of the text classification to work with\n",
    "print(f\"Amount of data samples used to train the initial model: {initial_data_amount}\")\n",
    "print(f\"Number of iterations in the active learning workflow: {n_iterations}\")\n",
    "print(f\"Number of data samples annotated in each iteration: {batch_size_to_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from encord_active.lib.metrics.acquisition_metrics.common import SKLearnClassificationModel\n",
    "\n",
    "def init_and_train_model(X, y):\n",
    "    forest = RandomForestClassifier(n_estimators = 500)\n",
    "    forest.fit(X, y)\n",
    "    return SKLearnClassificationModel(forest)\n",
    "\n",
    "def get_accuracy_score(wrapped_model, X, y):\n",
    "    return wrapped_model._model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the acquisition functions\n",
    "\n",
    "The acquisition functions purpose is to recommend what data to label next in each cycle of the active learning workflow.\n",
    "\n",
    "Choose from those already implemented in Encord Active or write your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encord_active.lib.metrics.acquisition_metrics.acquisition_functions import Entropy, LeastConfidence, Margin, Variance\n",
    "from encord_active.lib.metrics.heuristic.random import RandomImageMetric\n",
    "\n",
    "# use 'asc' (ascending) and 'desc' (descending) ordering for posterior selection of k highest ranked data samples\n",
    "selected_acq_funcs = [\n",
    "    (Entropy, \"desc\"),\n",
    "    (LeastConfidence, \"desc\"),\n",
    "    (Margin, \"asc\"),\n",
    "    (Variance, \"asc\"),\n",
    "    (RandomImageMetric, \"asc\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the active learning workflow\n",
    "\n",
    "Run simulations of the active learning workflow with each acquisition function.\n",
    "\n",
    "![active learning cycle](https://raw.githubusercontent.com/encord-team/encord-active/main/docs/docs/images/active-learning/active-learning-cycle.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from encord_active.lib.metrics.execute import execute_metrics\n",
    "from encord_active.lib.common.active_learning import get_data,    get_metric_results, get_n_best_ranked_data_samples\n",
    "\n",
    "def transform_image_data(images: List[Image]) -> List[np.ndarray]:\n",
    "    return [np.asarray(image).flatten() / 255 for image in images]\n",
    "\n",
    "accuracy_logger = defaultdict(dict)\n",
    "X_test, y_test = get_data(project_fs_test, data_hashes_test, class_name)\n",
    "X_test = transform_image_data(X_test)\n",
    "\n",
    "for acq_func, rank_order in selected_acq_funcs:\n",
    "    # Mockup of the initial labeling phase\n",
    "    labeled_data_hashes_train = data_hashes_train[:initial_data_amount]\n",
    "    unlabeled_data_hashes_train = set(data_hashes_train[initial_data_amount:])\n",
    "    \n",
    "    # Train the model\n",
    "    X, y = get_data(project_fs_train, labeled_data_hashes_train, class_name)\n",
    "    X = transform_image_data(X)\n",
    "\n",
    "    model = init_and_train_model(X, y)\n",
    "    \n",
    "    accuracy_logger[acq_func.__name__][0] = get_accuracy_score(model, X_test, y_test)\n",
    "    \n",
    "    for it in tqdm(range(1, n_iterations + 1), disable=False, desc=f\"Analyzing {acq_func.__name__} performance\"):\n",
    "        if acq_func.__name__ in [\"RandomImageMetric\"]:\n",
    "            acq_func_instance = acq_func()\n",
    "        else:\n",
    "            acq_func_instance = acq_func(model)\n",
    "        \n",
    "        # Run the acquisition function\n",
    "        execute_metrics([acq_func_instance], data_dir=project_fs_train.project_dir, use_cache_only=True)\n",
    "        \n",
    "        # Get the data scores\n",
    "        acq_func_results = get_metric_results(project_fs_train, acq_func_instance)\n",
    "        \n",
    "        # Select the data to label next \n",
    "        data_to_label_next, _ = get_n_best_ranked_data_samples(\n",
    "            acq_func_results, \n",
    "            batch_size_to_label, \n",
    "            rank_by=rank_order, \n",
    "            filter_by_data_hashes=unlabeled_data_hashes_train)\n",
    "           \n",
    "        # Mockup of the labeling phase\n",
    "        X_new, y_new = get_data(project_fs_train, SKLearnClassificationModel, data_to_label_next, class_name)\n",
    "        X.extend(X_new)\n",
    "        y.extend(y_new)\n",
    "        unlabeled_data_hashes_train.difference_update(data_to_label_next)\n",
    "        \n",
    "        # Train the model with the newly labeled data\n",
    "        model = init_and_train_model(X, y)\n",
    "        accuracy_logger[acq_func.__name__][it] = get_accuracy_score(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc: beautify function names\n",
    "if RandomImageMetric.__name__ in accuracy_logger:\n",
    "    accuracy_logger[\"Random\"] = accuracy_logger.pop(RandomImageMetric.__name__)\n",
    "if LeastConfidence.__name__ in accuracy_logger:\n",
    "    accuracy_logger[\"Least Confidence\"] = accuracy_logger.pop(LeastConfidence.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and visualize the simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for acq_func_name, points in accuracy_logger.items():\n",
    "    xs, ys = zip(*points.items())\n",
    "    plt.plot(xs, ys, label=acq_func_name)\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Model Accuracy\")\n",
    "plt.xticks(range(n_iterations + 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
